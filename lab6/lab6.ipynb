{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77b75a2",
   "metadata": {},
   "source": [
    "# Lab work №6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression:\n",
      "[[1770  261]\n",
      " [ 214 1755]]\n",
      "Accuracy for Logistic Regression: 0.88125\n",
      "\n",
      "Confusion Matrix for TextBlob:\n",
      "[[1528  503]\n",
      " [ 442 1527]]\n",
      "Accuracy for TextBlob: 0.76375\n",
      "\n",
      "Review: Picking this up along with the rest of the Marx Brothers box set, I found myself disappointed by most everything beyond A Night at the Opera. This stinker is prolly the worst I've seen of them so far, with the clever lines left out and the characterization is woeful. The playwright is so obscenely stupid in this play it's hard not to tackle the television and try and strangle him.<br /><br />As it is, the Marxes seem to do better as outsiders brought in to wreak havoc, and are much much better when they have a good gag or two at least. The material here is all obviously written for anyone, and it really wastes the Marx's talent. Avoid. <br /><br />Rating: 3/10\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment (Logistic Regression): 0\n",
      "Predicted Sentiment (TextBlob): 0\n",
      "\n",
      "Review: All those who are into the PC culture are aghast at the dogmatic Christian view of this film, claiming it contains racist ideation and/or religious intolerance.<br /><br />Those who don't care about this, but are oriented towards slick production values and competent acting are dismayed at the lack of such here.<br /><br />Those who decry both of these are apoplectic that this production was let loose on the general public, as evidenced in comments here.<br /><br />What is an interesting premise, which isn't original, but is a combination of GHOST and FROM BEYOND, is dealt with in a rather immature manner in this film, yet done with gusto. What the crew and actors lacked in sensibility, professional abilities and technical expertise is somewhat offset by the intensity they display.<br /><br />It isn't nearly as bad as many here think, and would have been fine in the hands of someone with maturity and common sense, and it is enough below mediocrity to elicit laughs and groans. However, it unfolds with enough intensity to keep interest throughout, and is close to on par with a Corman-produced entry of his earliest period of work, or the material of Arkoff or Sam Katzman. If you get it for $2 (or less) as did I, you won't feel disappointed, but will wish you could have had a say in how it was made.\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment (Logistic Regression): 0\n",
      "Predicted Sentiment (TextBlob): 0\n",
      "\n",
      "Review: ** Black Dragons (1942) William Nigh ~ Bela Lugosi, Joan Barclay, Clayton Moore <br /><br />\"Just prior to the start of World War II, Dr. Melcher (Bela Lugosi), a world-famous surgeon, is brought in by Japan's Black Dragon Society as part of a secret plan. Dr. Melcher operates on six Black Dragon Society operatives and transforms them into exact duplicates of 6 high ranking American businessmen who are replaced by these look-alikes. With their operatives in place, the Black Dragon Society's plan to sabotage the American war effort appears to be set but, the F.B.I. Chief and an agent begin to piece together the clues that hopefully uncover this sinister plot,\" according to the DVD sleeve's synopsis.<br /><br />That synopsis gives away the entire ending; which, in this case, might be a good thing. \"Black Dragons\" is an incredible, wildly inconsistent muddle. A wiser course of action would have been to stay with the teasing supernatural angle. In early scenes, Mr. Lugosi (as Monsieur Colomb) is effectively creepy. Confusing Joan Barclay (as Alice Saunders), future \"Lone Ranger\" Clayton Moore (as Dick Martin), along with a cast of old stage and silent veterans do the best they can with a story that looks as if filmmakers were making it up as they went along.\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment (Logistic Regression): 0\n",
      "Predicted Sentiment (TextBlob): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "\n",
    "# Reading data from the file movie1.csv\n",
    "data = pd.read_csv(\"movie1.csv\")\n",
    "\n",
    "# Preprocessing data and splitting into training and testing sets\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Training logistic regression model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Displaying confusion matrix and accuracy of the model\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy for Logistic Regression:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Sentiment analysis using TextBlob\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity >= 0.1:\n",
    "        return 1\n",
    "    elif polarity < 0.1:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# Evaluating sentiment using TextBlob\n",
    "y_pred_textblob = [analyze_sentiment(review) for review in X_test]\n",
    "\n",
    "# Displaying confusion matrix and accuracy of the model (TextBlob)\n",
    "print(\"\\nConfusion Matrix for TextBlob:\")\n",
    "print(confusion_matrix(y_test, y_pred_textblob))\n",
    "print(\"Accuracy for TextBlob:\", accuracy_score(y_test, y_pred_textblob))\n",
    "\n",
    "# Selecting three random entries and displaying sentiment evaluation results\n",
    "random_samples = random.sample(range(len(data)), 3)\n",
    "for idx in random_samples:\n",
    "    review = data.loc[idx, \"text\"]\n",
    "    actual_sentiment = data.loc[idx, \"label\"]\n",
    "    review_vectorized = vectorizer.transform([review])\n",
    "    predicted_sentiment_logistic = classifier.predict(review_vectorized)[0]\n",
    "    predicted_sentiment_textblob = analyze_sentiment(review)\n",
    "    print(\"\\nReview:\", review)\n",
    "    print(\"Actual Sentiment:\", actual_sentiment)\n",
    "    print(\"Predicted Sentiment (Logistic Regression):\", predicted_sentiment_logistic)\n",
    "    print(\"Predicted Sentiment (TextBlob):\", predicted_sentiment_textblob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475dc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) Words that are not stopwords: ['Gordon', 'Brown', 'issued', 'rallying', 'cry', 'telling', 'supporters', 'stakes', 'high', 'stay', 'home', 'protest', 'vote', 'forthcoming', 'general', 'election', 'chancellor', 'said', 'vote', 'expected', 'fall', 'clear', 'fundamental', 'choice', 'Labour', 'investment', 'Conservative', 'cuts', 'Speaking', 'Labour', 'spring', 'conference', 'Gateshead', 'Mr', 'Brown', 'claimed', 'NHS', 'safe', 'Conservative', 'hands', 'said', 'Tory', 'plans', 'cut', 'tax', 'cut', 'deep', 'public', 'service', 'packed', 'audience', 'Gateshead', 'Sage', 'Centre', 'chancellor', 'said', 'cuts', 'proposed', 'shadow', 'chancellor', 'Oliver', 'Letwin', 'equivalent', 'sacking', 'teacher', 'GP', 'nurse', 'country', 'told', 'activists', 'Laying', 'Conservative', 'record', 'government', 'said', 'promise', 'Labour', 'Britain', 'return', 'mistakes', 'ERM', 'inflation', 'interest', 'rates', 'lost', 'reserves', 'repossessed', 'million', 'negative', 'equity', 'million', 'unemployed', 'Tory', 'boom', 'bust', 'central', 'dividing', 'line', 'election', 'Conservative', 'Party', 'taking', 'Britain', 'planning', 'deep', 'cuts', 'services', 'Labour', 'government', 'taking', 'Britain', 'forward', 'platform', 'stability', 'reform', 'renew', 'hospitals', 'schools', 'public', 'services', 'proud', 'spend', 'Turning', 'economy', 'chancellor', 'pledged', 'continue', 'economic', 'stability', 'growth', 'term', 'power', 'said', 'seven', 'years', 'Labour', 'transformed', 'party', 'trusted', 'economy', 'party', 'trusted', 'economy', 'party', 'employees', 'employers', 'managers', 'said', 'speech', 'prompted', 'standing', 'ovation', 'audience', 'clearly', 'warm', 'Mr', 'Brown', 'promised', 'end', 'teenage', 'unemployment', 'years', 'highlighted', 'plans', 'debt', 'relief', 'world', 'poorest', 'countries', 'national', 'minimum', 'wage', 'year', 'olds', 'creation', 'network', 'children', 'centres', 'flexibility', 'maternity', 'leave', 'prime', 'minister', 'later', 'Saturday', 'interactive', 'question', 'answer', 'session', 'fielding', 'queries', 'sent', 'e', 'mail', 'text', 'message', 'telephone', 'Labour', 'attempt', 'engage', 'public', 'campaign']\n",
      "b) Verbs in the text: ['issued', 'rallying', 'telling', 'stay', 'said', 'expected', 'fall', 'give', 'Speaking', 'claimed', 'said', 'plans', 'cut', 'ВЈ35bn', 'cut', 'packed', 'said', 'proposed', 'sacking', 'told', 'Laying', 'said', 'give', 'return', 'lost', 'repossessed', 'taking', 'planning', 'taking', 'reform', 'renew', 'say', 'spend', 'ВЈ60bn', 'Turning', 'pledged', 'continue', 'said', 'transformed', 'trusted', 'trusted', 'said', 'prompted', 'promised', 'end', 'highlighted', 'take', 'fielding', 'sent', 'engage']\n",
      "c1) Numbers in the text: ['5', '250,000', 'one million', 'three million', '16']\n",
      "c2) Persons in the text: ['Gordon Brown', 'Brown', 'Tory', 'Oliver Letwin', 'Mr Brown']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Task a)\n",
    "def find_non_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    non_stopwords = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return non_stopwords\n",
    "\n",
    "# Task b)\n",
    "def find_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "    return verbs\n",
    "\n",
    "# Task c)\n",
    "def find_numbers_and_persons(text):\n",
    "    doc = nlp(text)\n",
    "    numbers = [entity.text for entity in doc.ents if entity.label_ == \"CARDINAL\"]\n",
    "    persons = [entity.text for entity in doc.ents if entity.label_ == \"PERSON\"]\n",
    "    return numbers, persons\n",
    "\n",
    "# Reading text from the file lab6-3.txt\n",
    "with open(\"lab6-3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "non_stopwords = find_non_stopwords(text)\n",
    "verbs = find_verbs(text)\n",
    "numbers, persons = find_numbers_and_persons(text)\n",
    "\n",
    "print(\"a) Words that are not stopwords:\", non_stopwords)\n",
    "print(\"b) Verbs in the text:\", verbs)\n",
    "print(\"c1) Numbers in the text:\", numbers)\n",
    "print(\"c2) Persons in the text:\", persons)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
