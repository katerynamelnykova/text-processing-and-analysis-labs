{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab work №5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of topics: 24\n",
      "\n",
      "Document 1 :\n",
      "Appleton Partners Senior Vice President Whitney Fitts says too many Federal Reserve rate hikes are “cooked” into markets.   Speaking with Taylor Riggs on “Bloomberg Markets: The Close,” she adds that she sees a lot of opportunities in municipal bonds  https://t.co/K60bTZDXl9  https://t.co/m7U5vRIBOi\n",
      "Most important topics:\n",
      "Topic 9 probability: 0.2220063\n",
      "Topic 14 probability: 0.36111036\n",
      "\n",
      "Document 2 :\n",
      "The global oil market is ‘walking a tightrope’ between scarce supply and the possibility of a recession, the International Energy Agency said, with higher prices and worsening economic conditions already taking a toll on demand  https://t.co/R5VV9ATIce\n",
      "Most important topics:\n",
      "Topic 14 probability: 0.39592344\n",
      "Topic 16 probability: 0.22615068\n",
      "\n",
      "Document 3 :\n",
      "Tesla CEO Elon Musk sees inflation declining 'towards the end of this year'  https://t.co/Au0sTZAbAY by @BrianSozzi\n",
      "Most important topics:\n",
      "Topic 4 probability: 0.33242756\n",
      "Topic 14 probability: 0.30837154\n",
      "\n",
      "Document 4 :\n",
      "Rishi Sunak and Penny Mordaunt secure the most votes in the third round of the Tory leadership race, with Tom Tugendhat eliminated  https://t.co/2jY4jiey2D\n",
      "Most important topics:\n",
      "Topic 22 probability: 0.25541416\n",
      "Topic 23 probability: 0.30070898\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "\n",
    "# Loading the data\n",
    "data = pd.read_csv('news.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Cleaning the text\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Creating dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data['cleaned_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['cleaned_text']]\n",
    "\n",
    "# Function to compute coherence metric\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Computing coherence metric for various numbers of topics\n",
    "limit = 25\n",
    "start = 2\n",
    "step = 1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=data['cleaned_text'], start=start, limit=limit, step=step)\n",
    "\n",
    "# Choosing the optimal number of topics\n",
    "optimal_num_topics = start + coherence_values.index(max(coherence_values))\n",
    "print(\"Optimal number of topics:\", optimal_num_topics)\n",
    "\n",
    "# Building the final model with the optimal number of topics\n",
    "final_model = model_list[coherence_values.index(max(coherence_values))]\n",
    "\n",
    "# Displaying the most important topics for four randomly selected documents\n",
    "for i in range(4):\n",
    "    random_index = random.randint(0, len(data) - 1)\n",
    "    random_document = data.loc[random_index, 'text']\n",
    "    print(\"\\nDocument\", i+1, \":\")\n",
    "    print(random_document)\n",
    "    print(\"Most important topics:\")\n",
    "    topics = final_model.get_document_topics(dictionary.doc2bow(clean_text(random_document)), minimum_probability=0.2)\n",
    "    for topic, prob in topics:\n",
    "        print(\"Topic\", topic, \"probability:\", prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key trigrams:\n",
      "said mock turtle - 20 occurrences\n",
      "said march hare - 10 occurrences\n",
      "said alice said - 8 occurrences\n",
      "march hare said - 6 occurrences\n",
      "mock turtle said - 6 occurrences\n",
      "little golden key - 5 occurrences\n",
      "poor little thing - 5 occurrences\n",
      "white kid glove - 5 occurrences\n",
      "certainly said alice - 4 occurrences\n",
      "know said alice - 4 occurrences\n",
      "might well say - 4 occurrences\n",
      "mouse mouse mouse - 4 occurrences\n",
      "join dance wo - 4 occurrences\n",
      "dance wo wo - 4 occurrences\n",
      "wo wo join - 4 occurrences\n",
      "wo join dance - 4 occurrences\n",
      "beau ootiful soo - 4 occurrences\n",
      "ootiful soo oop - 4 occurrences\n",
      "king white rabbit - 4 occurrences\n",
      "said white rabbit - 4 occurrences\n",
      "cat eat bat - 3 occurrences\n",
      "thought poor alice - 3 occurrences\n",
      "like said alice - 3 occurrences\n",
      "said alice rather - 3 occurrences\n",
      "much said alice - 3 occurrences\n",
      "indeed said alice - 3 occurrences\n",
      "alice looked round - 3 occurrences\n",
      "took hookah mouth - 3 occurrences\n",
      "said caterpillar alice - 3 occurrences\n",
      "old father william - 3 occurrences\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Loading the text \"Alice's Adventures in Wonderland\" from the Gutenberg corpus\n",
    "alice_text = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# Splitting the text into tokens and cleaning the text\n",
    "tokens = clean_text(alice_text)\n",
    "\n",
    "# Generating trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "# Calculating the frequency of each trigram\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# Displaying trigrams that are considered key (most frequent)\n",
    "print(\"Key trigrams:\")\n",
    "for trigram, freq in trigram_freq.most_common(30):\n",
    "    print(' '.join(trigram), \"-\", freq, \"occurrences\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
